---
title: "Join Enviro Variables for Tampa Bay"
output: html_document
---

```{r setup, include=FALSE}
require ("knitr")
knitr::opts_chunk$set(echo = TRUE)
opts_knit$set(root.dir ="U:/PhD_projectfiles/Raw_Data/Seatrout_FIM_Data/FIMData/NEWNov7" )
```

## About

This is an R Markdown document. 
This document details how environmental data not available within the FIM dataset are appended to the FIM catch data set. 

## Set File Locations ###

Can't use setwd for knitr so one is chosen as the root directory in the set up chunk and then if I have to reference other places then I will use other locations defined below. 
  
```{r}
personal_comp = "~/Desktop/PhD project/Projects/Seatrout/Data/Raw Survey Data/Seatrout FIM Data"
work_comp= "U:/PhD_projectfiles/Raw_Data/Seatrout_FIM_Data/FIMData/NEWNov7"
phys_dat = "U:/PhD_projectfiles/Raw_Data/Seatrout_FIM_Data/Raw Data from fimaster-data-sas-inshore"
nutrient_dat = "U:/PhD_projectfiles/Raw_Data/Environmental_Data/Nutrients"

```

## Load Packages 
* haven-to load sas7bdat data
* dplyr- to do df manipulation
* geosphere-  closest lat/long neighbors
* cluster- to determine medoids

```{r}
library(haven) 
library(dplyr) 
library(geosphere)
library(cluster)
```

## Import Data Sets
### About the Data
The fisheries independent monitoring data (FIM) contain positive and zero catch observations of Spotted seatrout for every single haul conducted in each estuary since sampling inception.  

Load FIM Data and select the important recruitment months for each zone so that we are only selecting young-of-the-year individuals. Also, there was a typo in variable name so adjust it to be consistent with other dataframes. 
```{r Load AP}
ap = subset(read_sas("ap_yoy_cn_c.sas7bdat"), month %in% c(6,7,8,9,10,11)) %>% mutate(bUnk=bunk) %>% select(-bunk) 
```
 Load the hydro dataset that contains Depth where the observation was taken, Temperature, Conductivity, pH, Salinity, Dissolved O2
```{r Load AP 2}
ap_hyd <- subset(read_sas("ap_yoy_cn_hyd.sas7bdat")) 
```
Load the physical data set that contains secchi depth, a proxy for turbidity and turn Reference into a character. Reference will be used as a matching variable. 

```{r Load AP 3}
ap_phys <- read_sas(paste(phys_dat, "apm_physical.sas7bdat", sep="/")) %>% select(Reference, Secchi_on_bottom, Secchi_depth)
ap_phys$Reference <- as.character(ap_phys$Reference)
```

Load the hydro dataset that contains Depth where the hydro observation was taken, Temperature, Conductivity, pH, Salinity, Dissolved O2.it appears at some stations more than 1 hydro observation was recorded so there are duplicate station records. Just select the unique ones.At this point, I have no idea to know which hydro observation is more "accurat" for a certain station. 

``` {r Load AP 4}
ap_hyd <- subset(read_sas("ap_yoy_cn_hyd.sas7bdat")) 
ap_hyd <- ap_hyd[!duplicated(ap_hyd$Reference),]
```
Match merge the catch dataframe (ap) with the physical data frame (ap_phys) using the variable Reference. Then again left_join in the hydro (ap_hyd) dataset by reference. Then reorder the columns alphabetically. 

```{r Load AP 5}
ap <- left_join(ap, ap_phys, by="Reference") %>% left_join(ap_hyd, by="Reference")
ap <- ap %>% select(noquote(order(colnames(ap))))  #reorders the columns alphabetically 
```

Do this for all other estuaries.

####TB
```{r LoadTB}
tb = subset(read_sas("tb_yoy_cn_c.sas7bdat"), month %in% c(4,5,6,7,8,9,10)) 
#tb_phys <- read_sas("~/Desktop/PhD project/Projects/Seatrout/Data/Raw Survey Data/Seatrout FIM Data/tbm_physical.sas7bdat") %>% select(Reference, Secchi_on_bottom, Secchi_depth)
tb_phys <- read_sas(paste(phys_dat, "tbm_physical.sas7bdat", sep="/")) %>% select(Reference, Secchi_on_bottom, Secchi_depth)
tb_phys$Reference <- as.character(tb_phys$Reference)
tb_hyd <- subset(read_sas("tb_yoy_cn_hyd.sas7bdat")) 
tb_hyd <- tb_hyd[!duplicated(tb_hyd$Reference),]
tb <- left_join(tb, tb_phys, by="Reference") %>% left_join(tb_hyd, by="Reference")
tb <- tb %>% select(noquote(order(colnames(tb))))  #reorders the columns alphabetically 
```

Clean TB. Add in missing lat and long based on the Zone. I.e. if lat and long is missing then use lat and long for similar zone. This is necessary to be able to match any environmental data based on time and space. 

```{r}
unique(tb$Zone)
unique(subset(tb, is.na(Longitude))$Zone) #assume if its missing Long then its also missing Lat
```
Determine Zone-based medoids to apply to reference values (aka hauls) that have missing Lat Longs

```{r Zone specific Medoids}

tb_medA <- matrix(pam(subset(tb, Zone=="A" & !is.na(Longitude) & !is.na(Latitude), select=c("Longitude", "Latitude")),1)$medoids)
sum(is.na(tb_medA))

tb_medC <- matrix(pam(subset(tb, Zone=="C" & !is.na(Longitude) & !is.na(Latitude), select=c("Longitude", "Latitude")),1)$medoids)
sum(is.na(tb_medC))

tb_medD <- matrix(pam(subset(tb, Zone=="D" & !is.na(Longitude) & !is.na(Latitude), select=c("Longitude", "Latitude")),1)$medoids)
sum(is.na(tb_medD))

tb_medE <- matrix(pam(subset(tb, Zone=="E" & !is.na(Longitude) & !is.na(Latitude), select=c("Longitude", "Latitude")),1)$medoids)
sum(is.na(tb_medE))

tb_medM <- matrix(pam(subset(tb, Zone=="M" & !is.na(Longitude) & !is.na(Latitude), select=c("Longitude", "Latitude")),1)$medoids)
sum(is.na(tb_medM))
```
Apply zone-specific medoids to missing Long and Lat. Do this with new varaibles NewLong and NewLat 
```{r}
TB_main <- tb %>% mutate(NewLong = ifelse(Zone == "A" & is.na(Longitude), tb_medA[1,],  ifelse(Zone=="C" & is.na(Longitude), tb_medC[1,], ifelse(Zone == "D" & is.na(Longitude), tb_medD[1,], ifelse(Zone=="E" & is.na(Longitude), tb_medE[1,], ifelse(Zone == "M" & is.na(Longitude), tb_medM[1,], tb$Longitude))))), NewLat = ifelse(Zone == "A" & is.na(Latitude), tb_medA[2,],  ifelse(Zone=="C" & is.na(Latitude), tb_medC[2,], ifelse(Zone == "D" & is.na(Latitude), tb_medD[2,], ifelse(Zone=="E" & is.na(Latitude), tb_medE[2,], ifelse(Zone == "M" & is.na(Latitude), tb_medM[2,], tb$Latitude))))))

TB_main$year <- as.numeric(TB_main$year)
TB_main$month <- as.numeric(TB_main$month)

sum(is.na(TB_main$NewLong))                         
sum(is.na(TB_main$NewLat))   
```
### Add in Enviro Data 
### Nitrogen
```{r Load TB Nitrogen}
tb_nit1 <- read.csv(paste(nutrient_dat, "Nitrogen_Hillsborough_Bay_EPC_Routine.csv", sep="/"))
tb_nit2 <- read.csv(paste(nutrient_dat, "Nitrogen_Middle_Lower_Tampa_Bay_EPC_Routine.csv", sep="/"))
tb_nit3 <- read.csv(paste(nutrient_dat, "Nitrogen_Old_Tampa_Bay_EPC_Routine.csv", sep="/"))
tb_nit <- rbind(tb_nit1, tb_nit2, tb_nit3)
```
Need to match the nitrogen observation spatially and temporally with the catch observations (to create observations at month of haul- monthly present observations) AS WELL as creating an average nitrogen load prior to 'recruitment' timeframe (to create time lagged). 

Get Year and Month in the right format to match to the TB_main dataframe. 
```{r Strip Time}
tb_nit$SampleDate <- as.factor(tb_nit$SampleDate)
tb_nit <- tb_nit %>% mutate(Date = as.Date(SampleDate, format = " %m/%d/%Y"))
tb_nit$Date <- as.character(tb_nit$Date)

tb_nit <- tb_nit %>% mutate(Year = substr(Date, 1,4), Month = substr(Date, 6,7)) %>% subset(Characteristic %in% c("Nitrogen") & (Year >1988 & Year<2016)) %>% select(Actual_Latitude, Actual_Longitude, Characteristic,Parameter,Result_Unit, Result_Value, Year, Month, StationID)
tb_nit$Year <- as.numeric(tb_nit$Year)
tb_nit$Month <- as.numeric(tb_nit$Month)
```

Match the nitrogen observation with catch observation based on year, month, and closest Lat and Long.Must have same year, same month, and closest Lat observation then append on nitrogen observation. Going to match lat and long by fuzzy matching and month and year exactly. 

Starting with lat and long fuzzy matching because it's trickier and requires some determination of the fuzzy match. 

```{r}
range(unique(tb_nit$Actual_Latitude)) #determine the total range of latitudes in the TB_nit
#27.5737 - 27.9904
range(unique(TB_main$NewLat)) #determine the total range of lat in the TB_main
# 27.466 -  28.03467

range(unique(tb_nit$Actual_Longitude)) #determine the total range of longs in the TB_nit
# -82.7441 - -82.4093
range(unique(TB_main$NewLong))
# -82.831 - -82.333
```
Determine difference between minimum lats of tb_nit and TB_main and difference between maximum lats of tb_nit and TB_main to determine how far or fuzzy things could be.
Do this same thing for longitude. 
```{r}
min(unique(tb_nit$Actual_Latitude)) - min(unique(TB_main$NewLat)) #0.1068669
max(unique(tb_nit$Actual_Latitude)) - max(unique(TB_main$NewLat)) #0.0442667

min(unique(tb_nit$Actual_Longitude)) - min(unique(TB_main$NewLong)) #0.0868976
max(unique(tb_nit$Actual_Longitude)) - max(unique(TB_main$NewLong)) #0.07563469
```
Fuzzy lat distance must be at least 0.1068669 or else the minimum lat observations from the TB_main will not be able to find a fuzzy match. In other words, a fuzzy match of less than that, say for the distance between maximum (0.044), will mean that the minimum lats will not find a match because they are separated by 0.1068669

Fuzzy long distance must be at least 0.0868976 per above logic. 

Run the loop using the defined fuzzy match.
```{r}
fuzzy_lat = 0.001 #max(min(unique(tb_nit$Actual_Latitude)) - min(unique(TB_main$NewLat)), max(unique(tb_nit$Actual_Latitude)) - max(unique(TB_main$NewLat)))

fuzzy_long = 0.001 # max(min(unique(tb_nit$Actual_Longitude)) - min(unique(TB_main$NewLong)),max(unique(tb_nit$Actual_Longitude)) - max(unique(TB_main$NewLong)))
TB_main_sub <- TB_main %>% subset(year  ==1989 & month ==8)
tb_nit_sub <- tb_nit %>% subset(Year == 1989 & Month ==  8)



for(i in 1:nrow(TB_main_sub))
  for(j in 1:nrow(tb_nit_sub))
    if((TB_main_sub[i,61] == tb_nit_sub[j,7]) & (TB_main_sub[i,30] == tb_nit_sub[j, 8])){
      fuzzymax_lat = TB_main_sub[i,65] +fuzzy_lat #TB_main_sub$NewLat
      fuzzymin_lat = TB_main_sub[i, 65] -fuzzy_lat
      fuzzymax_long = TB_main_sub[i, 64] + fuzzy_long #TBmain$NewLong
      fuzzymin_long = TB_main_sub[i, 64] - fuzzy_long}
else{NULL}
        if((tb_nit_sub[j, 1] > fuzzymin_lat) & (tb_nit_sub[j, 1] < fuzzymax_lat) & (tb_nit_sub[2]>fuzzymin_long) & (tb_nit_sub[2] < fuzzymax_long)) {
          #if((tb_nit_sub[2]>fuzzymin_long) & (tb_nit_sub[2] < fuzzymax_long)){
            TBcomb <- data.frame(Month=numeric())
            #Year=numeric(), refid=character(), nitrogen=numeric(), FIMlat=numeric(), FIMlong=numeric(), nitlat=numeric(), nitlong=numeric())}}
        
            
            TBcomb$Month = TB_main_sub[i,30]
        #TBcomb$Year = TB_main$year[i]
        #TBcomb$refid = TB_main$Reference[i]
        #TBcomb$nitrogen = tb_nit$Result_Value[j]
        #TBcomb$FIMlat = TB_main$NewLat[i]
        #TBcomb$FIMlong = TB_main$NewLong[i]
        #TBcomb$nitlat = TB_nit$Actual_Latitude[j]
        #TBcomb$nitlong =TB_nit$Actual_Longitude[j]
          }
      else{NULL}
    

   set1 <- structure(list(lon = c(13.671114, 12.866947, 15.94223, 11.099736,  
         12.958342, 14.203892, 11.86389, 16.526674, 16.193064, 17.071392
        ), lat = c(48.39167, 48.148056, 48.721111, 47.189167, 47.054443, 
         47.129166, 47.306667, 47.84, 47.304167, 48.109444)), .Names = c("lon", 
       "lat"), row.names = c(NA, 10L), class = "data.frame")

 set2 <- structure(list(lon = structure(c(14.4829998016357, 32.4000015258789, 
      -8.66600036621094, 15.4670000076294, 18.9160003662109, 19.0160007476807, 
      31.0990009307861, 14.3660001754761, 9.59899997711182, 11.0830001831055
       ), .Dim = 10L), lat = structure(c(35.8499984741211, 34.75, 70.9329986572266, 
      78.25, 69.6829986572266, 74.515998840332, 70.3659973144531, 67.265998840332, 
       63.6990013122559, 60.1990013122559), .Dim = 10L)), .Names = c("lon", 
      "lat"), row.names = c(NA, 10L), class = "data.frame")

 dd<- rdist.earth(set1,set2,miles=FALSE)



[stack overflow link][https://stackoverflow.com/questions/20590119/fuzzy-matching-of-coordinates]
rain <- structure(list(lat = c(-179.75, -179.75, -179.75, -179.75, -179.75, 
-179.75, -179.75, -179.75, -179.75, -179.75), lon = c(71.25, 
68.75, 68.25, 67.75, 67.25, 66.75, 66.25, 65.75, 65.25, -16.75
), rainfall = c(0, 4.9, 4.6, 4.9, 8.9, 15.2, 24.2, 16.3, 12.2, 
365.4)), .Names = c("lat", "lon", "rainfall"), class = "data.frame", row.names = c(NA, 
-10L))

addresses <- structure(list(address_lat = c(-175.33, -175.20, -177.65, -174.10, -175.80, 
-179.50, -179.23, -179.12, -178.75, -174.77), address_lon = c(70.25, 
69.75, 62.23, 60.50, 66.25, 61.75, 62.54, 63.70, 61.45, -15.80),
person_id = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)), .Names = c("address_lat", "address_lon",     
"person_id"), class = "data.frame", row.names = c(NA, -10L))

library(geosphere)
D = distm(addresses[, 1:2], rain[, 1:2])
new <- cbind(addresses, rain[apply(D, 1, which.min),])

test <- rain[apply(D, 1, which.min),]
#try with a shortened version (matching only over 1 variable )

tb_nit_shrt <- tb_nit %>% subset(Year == "1989")
TB_main <- TB_main %>% subset(year == "1989")




m <- "initialize"
for i in 1:length(tb_nit)
if(tb_nit$Year[i] = TB_main$year[i] & TB_main$month[i] = tb_nit$Month[i]{
  m[i] =cbind(TB_main[i], tb_nit[apply(distm(TB_main[,64:65], tb_nit[,7:8]))])
  
  
}
 



library(geosphere)
D = distm(tb_nit[,7:8], TB_main[,64:65])


cbind(addresses, rain[apply(D, 1, which.min),])



if(TB_main$month == tb_nit$)





```

library(geosphere)

      

CK
```{r Load CK, include =FALSE}
ck = subset(read_sas("ck_yoy_cn_c.sas7bdat"),  month %in% c(5,6,7,8,9,10,11))
#ck_phys <- read_sas("~/Desktop/PhD project/Projects/Seatrout/Data/Raw Survey Data/Seatrout FIM Data/ckm_physical.sas7bdat") %>% select(Reference, Secchi_on_bottom, Secchi_depth)
ck_phys <- read_sas(paste(phys_dat, "ckm_physical.sas7bdat", sep="/")) %>% select(Reference, Secchi_on_bottom, Secchi_depth)
ck_phys$Reference <- as.character(ck_phys$Reference)
ck_hyd <- subset(read_sas("ck_yoy_cn_hyd.sas7bdat")) 
ck_hyd <- ck_hyd[!duplicated(ck_hyd$Reference),]
ck <- left_join(ck, ck_phys, by="Reference") %>% left_join(ck_hyd, by="Reference")
ck <- ck %>% select(noquote(order(colnames(ck))))  #reorders the columns alphabetically 
```
             
                         
CH
``` {r Load CH, include=FALSE}
ch = subset(read_sas("ch_yoy_cn_c.sas7bdat"), month %in% c(4,5,6,7,8,9,10)) %>% mutate(bUnk=bunk) %>% select(-bunk) 
#ch_phys <- read_sas("~/Desktop/PhD project/Projects/Seatrout/Data/Raw Survey Data/Seatrout FIM Data/chm_physical.sas7bdat") %>% select(Reference, Secchi_on_bottom, Secchi_depth)
ch_phys <- read_sas(paste(phys_dat, "chm_physical.sas7bdat", sep="/")) %>% select(Reference, Secchi_on_bottom, Secchi_depth)
ch_phys$Reference <- as.character(ch_phys$Reference)
ch_hyd <- subset(read_sas("ch_yoy_cn_hyd.sas7bdat")) 
#Also duplicated References in ch_hyd
ch_hyd <- ch_hyd[!duplicated(ch_hyd$Reference),]
ch <- left_join(ch, ch_phys, by="Reference") %>% left_join(ch_hyd, by="Reference")
ch <- ch %>% select(noquote(order(colnames(ch))))  #reorders the columns alphabetically 
```

IR
```{r Load IR, include=FALSE}
ir = subset(read_sas("ir_yoy_cn_c.sas7bdat"), month %in% c(5,6,7,8,9,10,11)) 
#ir_phys <- read_sas("~/Desktop/PhD project/Projects/Seatrout/Data/Raw Survey Data/Seatrout FIM Data/irm_physical.sas7bdat") %>% select(Reference, Secchi_on_bottom, Secchi_depth)
ir_phys <- read_sas(paste(phys_dat, "irm_physical.sas7bdat", sep="/")) %>% select(Reference, Secchi_on_bottom, Secchi_depth)
ir_phys$Reference <- as.character(ir_phys$Reference)
ir_hyd <- subset(read_sas("ir_yoy_cn_hyd.sas7bdat")) 
#Also duplicated References in ir_hyd
ir_hyd <- ir_hyd[!duplicated(ir_hyd$Reference),]
ir <- left_join(ir, ir_phys, by="Reference") %>% left_join(ir_hyd, by="Reference")
ir <- ir %>% select(noquote(order(colnames(ir))))  #reorders the columns alphabetically 
```

JX
```{r Load JX, include=FALSE}
jx = subset(read_sas("jx_yoy_cn_c.sas7bdat") , month %in% c(5,6,7,8,9,10,11)) 
#jx_phys <- read_sas("~/Desktop/PhD project/Projects/Seatrout/Data/Raw Survey Data/Seatrout FIM Data/jxm_physical.sas7bdat") %>% select(Reference, Secchi_on_bottom, Secchi_depth)
jx_phys <- read_sas(paste(phys_dat, "jxm_physical.sas7bdat", sep="/")) %>% select(Reference, Secchi_on_bottom, Secchi_depth)
jx_phys$Reference <- as.character(jx_phys$Reference)
jx_hyd <- subset(read_sas("jx_yoy_cn_hyd.sas7bdat")) 
jx_hyd <- jx_hyd[!duplicated(jx_hyd$Reference),]
jx <- left_join(jx, jx_phys, by="Reference") %>% left_join(jx_hyd, by="Reference")
jx <- jx %>% select(noquote(order(colnames(jx))))  #reorders the columns alphabetically 
```


## Add in Enviro Data 
### Nitrogen

``` {r Load TB Nitrogen}
tb_nit1 <- read.csv(paste(nutrient_dat, "Nitrogen_Hillsborough_Bay_EPC_Routine.csv", sep="/"))
tb_nit2 <- read.csv(paste(nutrient_dat, "Nitrogen_Middle_Lower_Tampa_Bay_EPC_Routine.csv", sep="/"))
tb_nit3 <- read.csv(paste(nutrient_dat, "Nitrogen_Old_Tampa_Bay_EPC_Routine.csv", sep="/"))
tb_nit <- rbind(tb_nit1, tb_nit2, tb_nit3)
```

Need to match the nitrogen observation spatially and temporally with the catch observations (to create observations at month of haul- monthly present observations) AS WELL as creating an average nitrogen load prior to 'recruitment' timeframe (to create time lagged). 

First, present observations. Match by year and sampling month. Use strp time. 

```{r Strip Time}
tb_nit$SampleDate <- as.factor(tb_nit$SampleDate)
tb_nit <- tb_nit %>% mutate(Date = as.Date(SampleDate, format = " %m/%d/%Y"))
tb_nit$Date <- as.character(tb_nit$Date)
tb_nit <- tb_nit %>% mutate(Year = substr(Date, 1,4), Month = substr(Date, 6,7)) %>% select(-SampleDate)

library(geosphere)
D = distm(addresses[,1:2], rain[,1:2])
cbind(addresses, rain[apply(D, 1, which.min),])

```

library(geosphere)




### Phosphorous

```{r}
tb_p1 <- read.csv(paste(nutrient_dat, "Phosphorous_Hillsborough_Bay_EPC_Routine.csv", sep="/"))
tb_p2 <- read.csv(paste(nutrient_dat, "Phosphorous_Lower_Tampa_Bay_EPC_Routine.csv", sep="/"))
tb_p3 <- read.csv(paste(nutrient_dat, "Phosphorous_Middle_Tampa_Bay_EPC_Routine.csv", sep="/"))
tb_p4 <- read.csv(paste(nutrient_dat, "Phosphorous_Old_Tampa_Bay_EPC_Routine.csv", sep="/"))
```














## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
